{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 4\n",
    "# Regularization, High Dimensionality, PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately):\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.regression.linear_model as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.regression.linear_model as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on multiple linear regression, regularization, dealing with high dimensionality, and PCA. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See Homework 3 for more information about the data.\n",
    "\n",
    "*Note: please make sure you use all the processed data from HW 3 Part (a)...you make want to save the data set on your computer and reread the csv/json file here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('Bikeshare_train.csv', index_col=0)\n",
    "test = pd.read_csv('Bikeshare_test.csv', index_col=0)\n",
    "cat_preds = ['season', 'month', 'day_of_week', 'weather']\n",
    "labels = [['summer', 'fall', 'winter'],['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul','Aug','Sep','Oct','Nov','Dec'], \n",
    "          ['mon', 'tue', 'wed','thu', 'fri','sat'], ['cloud', 'light']]\n",
    "\n",
    "# loop through categorical variables and convert to one-hot encoding\n",
    "for i,pred in enumerate(cat_preds): \n",
    "    dum = pd.get_dummies(train[pred])\n",
    "    del dum [1]\n",
    "    dum.columns = labels[i]\n",
    "    train = train.join(dum)\n",
    "    del train[pred]\n",
    "\n",
    "# Repeat the same process for test data\n",
    "for i,pred in enumerate(cat_preds): \n",
    "    dum = pd.get_dummies(test[pred])\n",
    "    del dum [1]\n",
    "    dum.columns = labels[i]\n",
    "    test = test.join(dum)\n",
    "    del test[pred]\n",
    "    \n",
    "# Mean and STD of continuous preidctors based on trainig set\n",
    "temp_std = train['temp'].std(axis = 0)\n",
    "temp_mean = train['temp'].mean(axis = 0)\n",
    "atemp_std = train['atemp'].std(axis = 0)\n",
    "atemp_mean = train['atemp'].mean(axis = 0)\n",
    "humidity_std = train['humidity'].std(axis = 0)\n",
    "humidity_mean = train['humidity'].mean(axis = 0)\n",
    "wind_std = train['windspeed'].std(axis = 0)\n",
    "wind_mean = train['windspeed'].mean(axis = 0)\n",
    "\n",
    "# normalize training data\n",
    "train['temp'] = (train['temp'] - temp_mean) / temp_std\n",
    "train['atemp'] = (train['atemp'] - atemp_mean) / atemp_std\n",
    "train['humidity'] = (train['humidity'] - humidity_mean) / humidity_std\n",
    "train['windspeed'] = (train['windspeed'] - wind_mean) / wind_std\n",
    "# normalize test data\n",
    "test['temp'] = (test['temp'] - temp_mean) / temp_std\n",
    "test['atemp'] = (test['atemp'] - atemp_mean) / atemp_std\n",
    "test['humidity'] = (test['humidity'] - humidity_mean) / humidity_std\n",
    "test['windspeed'] = (test['windspeed'] - wind_mean) / wind_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed',\n",
      "       'summer', 'fall', 'winter', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul',\n",
      "       'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'mon', 'tue', 'wed', 'thu', 'fri',\n",
      "       'sat', 'cloud', 'light'],\n",
      "      dtype='object')\n",
      "Index(['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed',\n",
      "       'summer', 'fall', 'winter', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul',\n",
      "       'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'mon', 'tue', 'wed', 'thu', 'fri',\n",
      "       'sat', 'cloud', 'light'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y_train = train['count']\n",
    "# Get data for all predictors\n",
    "x = list(train.columns)\n",
    "x.remove('count')\n",
    "x_train = train[x]\n",
    "print(x_train.columns)\n",
    "\n",
    "y_test = test['count']\n",
    "# Get data for all predictors\n",
    "x = list(test.columns)\n",
    "x.remove('count')\n",
    "x_test = test[x]\n",
    "print(x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>count</th>\n",
       "      <th>summer</th>\n",
       "      <th>fall</th>\n",
       "      <th>winter</th>\n",
       "      <th>...</th>\n",
       "      <th>Nov</th>\n",
       "      <th>Dec</th>\n",
       "      <th>mon</th>\n",
       "      <th>tue</th>\n",
       "      <th>wed</th>\n",
       "      <th>thu</th>\n",
       "      <th>fri</th>\n",
       "      <th>sat</th>\n",
       "      <th>cloud</th>\n",
       "      <th>light</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623798</td>\n",
       "      <td>0.650106</td>\n",
       "      <td>0.920664</td>\n",
       "      <td>-0.928758</td>\n",
       "      <td>6073.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.180310</td>\n",
       "      <td>-0.054759</td>\n",
       "      <td>0.696852</td>\n",
       "      <td>-0.213502</td>\n",
       "      <td>6606.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802489</td>\n",
       "      <td>0.851495</td>\n",
       "      <td>-0.448383</td>\n",
       "      <td>0.803926</td>\n",
       "      <td>7363.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.520492</td>\n",
       "      <td>-1.565182</td>\n",
       "      <td>-0.332113</td>\n",
       "      <td>-0.269099</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.534453</td>\n",
       "      <td>0.348021</td>\n",
       "      <td>1.975789</td>\n",
       "      <td>-1.199027</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday  workingday      temp     atemp  humidity  windspeed   count  \\\n",
       "0      0.0         1.0  0.623798  0.650106  0.920664  -0.928758  6073.0   \n",
       "1      0.0         1.0 -0.180310 -0.054759  0.696852  -0.213502  6606.0   \n",
       "2      0.0         1.0  0.802489  0.851495 -0.448383   0.803926  7363.0   \n",
       "3      0.0         0.0 -1.520492 -1.565182 -0.332113  -0.269099  2431.0   \n",
       "4      0.0         1.0  0.534453  0.348021  1.975789  -1.199027  1996.0   \n",
       "\n",
       "   summer  fall  winter  ...    Nov  Dec  mon  tue  wed  thu  fri  sat  cloud  \\\n",
       "0       1     0       0  ...      0    0    0    1    0    0    0    0      1   \n",
       "1       0     0       1  ...      0    1    0    1    0    0    0    0      0   \n",
       "2       1     0       0  ...      0    0    0    0    0    1    0    0      0   \n",
       "3       0     0       1  ...      0    1    1    0    0    0    0    0      0   \n",
       "4       0     1       0  ...      0    0    0    0    1    0    0    0      0   \n",
       "\n",
       "   light  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Regularization/Penalization Methods\n",
    "\n",
    "As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "Use the following regularization techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "    \n",
    "You may choose the shrikage parameter $\\lambda$ from the set $\\{10^{-5}, 10^{-4},...,10^{4},10^{5}\\}$ using cross-validation. In each case, \n",
    "\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n",
    "We next analyze the performance of the two shrinkage methods for different training sample sizes:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIDGE\n",
      "Ridge\n",
      "Lambda =  1e-05   Test R2 =   0.249342182912\n",
      "Coefficients: [ -468.46678635   124.04186362   925.73195625   312.43496903  -548.49306923\n",
      "  -255.12261663   898.82426773  1032.87469071  1226.18213967    88.9412038\n",
      "   239.1839445    333.35559126   -65.80497803  -792.25406882 -1279.97614961\n",
      "  -776.46482545   405.15341993   486.25723467   112.68717398  -118.83192646\n",
      "   -60.35988713   -71.53430224   294.26265235   185.0077752    234.81849096\n",
      "   404.78480066   -16.56641837 -1581.97556793]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  0.0001   Test R2 =   0.249342825815\n",
      "Coefficients: [ -468.46050266   124.04125062   925.71491642   312.44304299  -548.49415086\n",
      "  -255.12286539   898.78132931  1032.81274274  1226.14251974    88.95120007\n",
      "   239.21135196   333.41412149   -65.73729529  -792.17461825 -1279.87845875\n",
      "  -776.36886386   405.23308805   486.3142007    112.73692142  -118.79690449\n",
      "   -60.36126874   -71.53291672   294.26111401   185.01019472   234.81952713\n",
      "   404.7805199    -16.56492747 -1581.95112768]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  0.001   Test R2 =   0.249349242933\n",
      "Coefficients: [ -468.39774867   124.03510774   925.54477741   312.52375146  -548.50497562\n",
      "  -255.12535554   898.35267599  1032.19418067  1225.74700404    89.0507851\n",
      "   239.4846972    333.99810068   -65.06193967  -791.38175439 -1278.90342708\n",
      "  -775.41106921   406.02815802   486.88257685   113.23327053  -118.44750512\n",
      "   -60.37509352   -71.51910022   294.24571341   185.0343041    234.82985002\n",
      "   404.73773436   -16.55001076 -1581.70681177]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  0.01   Test R2 =   0.249412251001\n",
      "Coefficients: [ -467.77632206   123.97443576   923.86881565   313.3277538   -548.61404068\n",
      "  -255.1505017    894.13778737  1026.09855219  1221.85886214    90.00966079\n",
      "   242.14671693   339.70827421   -58.45256576  -783.61397593 -1269.33704795\n",
      "  -766.01162145   413.82095595   492.44052779   118.08651039  -115.03388032\n",
      "   -60.51217708   -71.38472471   294.09003083   185.26699336   234.92927844\n",
      "   404.31406337   -16.40010251 -1579.27220293]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  0.1   Test R2 =   0.249948496575\n",
      "Coefficients: [ -462.07358408   123.4307237    909.2337149    321.0869373   -549.76596559\n",
      "  -255.42105447   857.94986474   972.67206596  1188.56065338    96.56299686\n",
      "   262.87159666   386.08139769    -4.29719428  -719.26549912 -1188.9503251\n",
      "  -686.85197285   478.63513288   537.59540944   157.48474617   -87.54913358\n",
      "   -61.78527833   -70.3546111    292.39940647   186.90068061   235.60817919\n",
      "   400.42813871   -14.84721654 -1555.69462054]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  1   Test R2 =   0.252418374525\n",
      "Coefficients: [ -4.23494723e+02   1.20063855e+02   8.37372734e+02   3.81381396e+02\n",
      "  -5.61113521e+02  -2.58341647e+02   6.97538436e+02   7.05534743e+02\n",
      "   1.04550677e+03   7.29983493e+01   2.86712713e+02   5.05672248e+02\n",
      "   1.52819387e+02  -5.08005857e+02  -8.91187983e+02  -3.89727073e+02\n",
      "   6.92882761e+02   6.52324318e+02   2.56913997e+02  -2.50105386e+01\n",
      "  -7.08907166e+01  -6.99237937e+01   2.72966080e+02   1.81932805e+02\n",
      "   2.32167372e+02   3.74321584e+02  -1.26757526e-01  -1.36488655e+03]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  10   Test R2 =   0.259532749973\n",
      "Coefficients: [-252.29664197   99.4047754   685.01394079  550.02634774 -570.64113284\n",
      " -267.0536033   387.1113081   175.48370635  767.39717706 -118.29008923\n",
      "   88.03377814  371.63135504  142.23359271 -310.6286024  -533.71668821\n",
      "  -91.70312439  678.42652182  502.09426129  155.64222645 -103.00157479\n",
      " -105.18408991  -75.11088464  174.49442759  113.97997918  177.22386257\n",
      "  258.07595648   20.88159619 -671.46824534]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  100   Test R2 =   0.263581151112\n",
      "Coefficients: [ -49.73811031   47.69829163  512.42039899  514.2577354  -370.43553363\n",
      " -227.97940322  127.50473409   31.55184243  291.24385423 -101.67954914\n",
      "  -33.05023685   89.70571374   57.51178462  -40.26661444 -132.6744868\n",
      "    1.70799392  224.31391085  182.35236379   46.11696504  -71.27999052\n",
      "  -67.13718041  -26.03062206   50.21164812   33.44593425   60.43403463\n",
      "   69.17699909  -76.96535039 -154.00984251]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  1000   Test R2 =   0.180739225779\n",
      "Coefficients: [  -4.83993942   10.4802024   224.9739191   226.95919772  -85.53016813\n",
      "  -85.60786797   25.14804653   47.9698353    26.95206457  -25.48145136\n",
      "  -14.36665946    6.85624936   15.21937554    9.44904095    4.43816552\n",
      "   13.68403473   36.31363912   24.56630744   -2.14882742  -20.72154137\n",
      "  -11.27422618   -4.64363941    9.16403117    5.32947461    8.4125812\n",
      "    5.63396321  -39.71570868  -24.47659713]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  10000   Test R2 =   0.0350351229215\n",
      "Coefficients: [ -0.44641924   1.42166268  35.62757349  35.8524986   -8.84203376\n",
      " -12.60293353   3.20435787   8.75760248   1.49216253  -3.51293702\n",
      "  -2.15714957   0.41670095   2.20164574   1.88781453   2.17371507\n",
      "   2.54351647   4.49701826   2.42229275  -1.06910561  -2.99228015\n",
      "  -1.31950137  -0.61019895   1.25776209   0.60968454   0.84497129\n",
      "   0.34425793  -5.45098582  -2.80592283]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Ridge\n",
      "Lambda =  100000   Test R2 =   0.00241548781955\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n"
     ]
    }
   ],
   "source": [
    "## RS NOTE THIS ISNT USING x VALIDATION...\n",
    "alpha= [-5, -4, -3, -2, -1, 0, 1, 2,3, 4, 5]\n",
    "train_r_squared = np.zeros(len(alpha))\n",
    "test_r_squared = np.zeros(len(alpha))\n",
    "\n",
    "print(\"RIDGE\")\n",
    "\n",
    "for i, alpha in enumerate(alpha):\n",
    "    reg = Ridge_Reg(10**alpha)\n",
    "    reg.fit(x_train, y_train)\n",
    "    coefficients = reg.coef_\n",
    "    # calculate r2 for testing and training sets\n",
    "    test_r_squared[i] = reg.score(x_test, y_test)\n",
    "    train_r_squared[i] = reg.score(x_train, y_train)\n",
    "    print('Ridge')   \n",
    "    print('Lambda = ', 10**alpha, '  Test R2 =  ', reg.score(x_test, y_test))\n",
    "    print('Coefficients:', coefficients)\n",
    "    print('Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0])\n",
    "    print('predictors w/ a coefficient value close to 0:', [i for i, item in enumerate(coefficients) if abs(item) < .0000000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO\n",
      "Lasso\n",
      "Lambda =  1e-05   Test R2 =   0.249342144656\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  0.0001   Test R2 =   0.249342443333\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  0.001   Test R2 =   0.249345426375\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  0.01   Test R2 =   0.249375217987\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  0.1   Test R2 =   0.249670992843\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  1   Test R2 =   0.252464082047\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  10   Test R2 =   0.266873491893\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  100   Test R2 =   0.247328747663\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  1000   Test R2 =   0.0662704774565\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  10000   Test R2 =   -0.00162440142643\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n",
      "Lasso\n",
      "Lambda =  100000   Test R2 =   -0.00162440142643\n",
      "Coefficients: [-0.04399607  0.14892471  3.78741153  3.81017076 -0.87386525 -1.3282172\n",
      "  0.33173557  0.94424731  0.12740058 -0.36766736 -0.227927    0.0369306\n",
      "  0.23197668  0.20436411  0.24731069  0.27464861  0.46465641  0.24101608\n",
      " -0.12217216 -0.31463698 -0.13530467 -0.06345241  0.13201638  0.06221797\n",
      "  0.08411674  0.03037603 -0.5658001  -0.28563892]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "predictors w/ a coefficient value close to 0: []\n"
     ]
    }
   ],
   "source": [
    "alpha= [-5, -4, -3, -2, -1, 0, 1, 2,3, 4, 5]\n",
    "train_r_squared = np.zeros(len(alpha))\n",
    "test_r_squared = np.zeros(len(alpha))\n",
    "print(\"LASSO\")\n",
    "for i, alpha in enumerate(alpha):\n",
    "    reg = Lasso_Reg(10**alpha)\n",
    "    reg.fit(x_train, y_train)\n",
    "    # calculate r2 for testing and training sets\n",
    "    test_r_squared[i] = reg.score(x_test, y_test)\n",
    "    train_r_squared[i] = reg.score(x_train, y_train)\n",
    "    print(\"Lasso\")\n",
    "    print('Lambda = ', 10**alpha, '  Test R2 =  ', reg.score(x_test, y_test))\n",
    "    print('Coefficients:', coefficients)\n",
    "    print('Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0])\n",
    "    print('predictors w/ a coefficient value close to 0:', [i for i, item in enumerate(coefficients) if abs(item) < .0000000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def sample(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    \n",
    "    return (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:\n",
    "- Fit linear, Ridge and Lasso regression models to each of the generated sample. In each case, compute the $R^2$ score for the model on the training sample on which it was fitted, and on the test set.\n",
    "- Repeat the above experiment for 10 random trials/splits, and compute the average train and test $R^2$ across the trials for each training sample size. Also, compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the linear, Ridge and Lasso regression methods as a function of the training sample size. Also, show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**. Make a similar plot for the test $R^2$ scores.\n",
    "\n",
    "How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values. You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 150, 200, 250, 300, 350, 400]\n"
     ]
    }
   ],
   "source": [
    "sample_size = list(range(100,450,50))\n",
    "print(sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Lin_Reg()\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "# score the model against the test and train sets\n",
    "test_r_squared_plain = reg.score(x_test, y_test)\n",
    "train_r_squared_plain = reg.score(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RS This code should help us plot the work later. needs to be adapted to include Ridge and lasso too. \n",
    "\n",
    "def PlotR2(lambdas, test_r_squared_plain, train_r_squared, test_r_squared):\n",
    "    # Plot train and test R-squared as a function parameter value\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    \n",
    "    # plain regression R2\n",
    "    ax.axhline(y = test_r_squared_plain, c='g', label = 'Plain Regression')\n",
    "    \n",
    "    # test and train R2\n",
    "    ax.semilogx(10.0**lambdas, (train_r_squared), c='b', label='Ridge: Train')\n",
    "    ax.semilogx(10.0**lambdas, (test_r_squared), c='r', label='Ridge: Test')\n",
    "\n",
    "    # add labels \n",
    "    ax.set_xlabel('Regularization parameter $\\lambda$')\n",
    "    ax.set_ylabel(r'$R^2$ score')\n",
    "    ax.legend(loc = 'best')\n",
    "\n",
    "    print 'Regression: max R^2 score on training set', max(train_r_squared)\n",
    "    print 'Regression: max R^2 score on test set', max(test_r_squared)\n",
    "    plt.show()\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Polynomial & Interaction Terms\n",
    "\n",
    "Moving beyond linear models, we will now try to improve the performance of the regression model in Part (b) from HW 3 by including higher-order polynomial and interaction terms. \n",
    "\n",
    "- For each continuous predictor $X_j$, include additional polynomial terms $X^2_j$, $X^3_j$, and $X^4_j$, and fit a multiple regression model to the expanded training set. How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "- Fit a multiple linear regression model with additional interaction terms $\\mathbb{I}_{month = 12} \\times temp$ and $\\mathbb{I}_{workingday = 1} \\times \\mathbb{I}_{weathersit = 1}$ and report the test $R^2$ for the fitted model. How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS: NOTES ARE HERE: https://github.com/cs109/a-2017/blob/40446caa5cea644068c564fb5e2dd06163a838cf/Lectures/Lecture4-IntroRegression/Lecture5_Notebook.ipynb\n",
    "\n",
    "y_train = train['count'].values,\n",
    "X_train = train[['temp', 'atemp', 'humidity', 'windspeed']].values\n",
    "\n",
    "a1= (X_train)\n",
    "a2= (X_train**2)\n",
    "a3= (X_train**3)\n",
    "a4= (X_train**4)\n",
    "all_as = np.append([a1], [a2])\n",
    "#all_as = np.append(all_as, a3)\n",
    "#all_as = np.append(all_as, a4)\n",
    "y_test = test['count'].values\n",
    "X_test = test[['temp', 'atemp', 'humidity', 'windspeed']].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_as.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train=X_train\n",
    "# Create the PolynomialFeatures object and specify the number of degrees:\n",
    "#degree = 2\n",
    "#poly = PolynomialFeatures(degree)\n",
    "\n",
    "x_train_poly = poly.fit_transform(all_as)\n",
    "x_test_poly = poly.fit_transform(x_test)\n",
    "pd.DataFrame(x_train_poly).shape\n",
    "\n",
    "# Create a linear regression object\n",
    "lg = LinearRegression()\n",
    "\n",
    "# Fit our training data with polynomial features \n",
    "final =lg.fit(x_train_poly, y_train)\n",
    "\n",
    "#final.summary()\n",
    "\n",
    "#print lg.predict(x_test_poly)\n",
    "#############\n",
    "# Obtain coefficients\n",
    "len(lg.coef_)\n",
    "\n",
    "#poly.predicted = lg.predict(x_test_poly)\n",
    "#(poly.predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_cross_terms = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "cross_terms = gen_cross_terms.fit_transform(X_train)\n",
    "X_train_with_cross = np.hstack((X_train, cross_terms))\n",
    "cross_terms = gen_cross_terms.fit_transform(X_test)\n",
    "X_test_with_cross = np.hstack((X_test, cross_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\"\"\"def make_features(train_set, test_set, degrees):\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    for d in degrees:\n",
    "        traintestdict={}\n",
    "        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n",
    "        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n",
    "    return train_dict, test_dict\n",
    "\n",
    "degrees=range(21)\n",
    "train_dict, test_dict = make_features(x_train, x_test, degrees)\n",
    "\n",
    "degrees=[1,2,3]\n",
    "Xtrain= x_train\n",
    "Xtest = x_test\n",
    "ytrain = y_train\n",
    "#for each degree, we now fit on the training set and predict on the test set\n",
    "#we accumulate the MSE on both sets in error_train and error_test\n",
    "for d in degrees:#for increasing polynomial degrees 0,1,2...\n",
    "    Xtrain = train_dict[d]\n",
    "    Xtest = test_dict[d]\n",
    "    #set up model\n",
    "    est = LinearRegression()\n",
    "    #fit\n",
    "    est.fit(Xtrain, ytrain)\n",
    "    #predict\n",
    "    #your code here\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): PCA to deal with high dimensionality\n",
    "\n",
    "We would like to fit a model to include all main effects, polynomial terms up to the $4^{th}$ order, and all interactions between all possible predictors and polynomial terms (not including the interactions between $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ as they would just create higher order polynomial terms).  \n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Beyond Squared Error\n",
    "\n",
    "We have seen in class that the multiple linear regression method optimizes the Mean Squared Error (MSE) on the training set. Consider the following alternate evaluation metric, referred to as the Root Mean Squared Logarthmic Error (RMSLE):\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (log(y_i+1) - log(\\hat{y}_i+1))^2}.\n",
    "$$\n",
    "\n",
    "The *lower* the RMSLE the *better* is the performance of a model. The RMSLE penalizes errors on smaller responses more heavily than errors on larger responses. For example, the RMSLE penalizes a prediction of $\\hat{y} = 15$ for a true response of $y=10$ more heavily than a prediction of $\\hat{y} = 105$ for a true response of $100$, though the difference in predicted and true responses are the same in both cases. \n",
    "\n",
    "This is a natural evaluation metric for bike share demand prediction, as in this application, it is more important that the prediction model is accurate on days where the demand is low (so that the few customers who arrive are served satisfactorily), compared to days on which the demand is high (when it is less damaging to lose out on some customers).\n",
    "\n",
    "The following code computes the RMSLE for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  rmsle\n",
    "# A function for evaluating Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "#      y_pred (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      RMSLE (float) \n",
    "\n",
    "def rmsle(y, y_pred):     \n",
    "    # Evaluate sqaured error, against target labels\n",
    "    # rmsle = \\sqrt(1/n \\sum_i (log (y[i]+1) - log (y_pred[i]+1))^2)\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    \n",
    "    return rmsle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g). \n",
    "\n",
    "You are required to develop a strategy to fit a regression model by optimizing the RMSLE on the training set. Give a justification for your proposed approach. Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? How about the test RMSLE of the new model? \n",
    "\n",
    "**Note:** We do not require you to implement a new regression solver for RMSLE. Instead, we ask you to think about ways to use existing built-in functions to fit a model that performs well on RMSLE. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (j): Dealing with Erroneous Labels\n",
    "\n",
    "Due to occasional system crashes, some of the bike counts reported in the data set have been recorded manually. These counts are not very unreliable and are prone to errors. It is known that roughly 5% of the labels in the training set are erroneous (i.e. can be arbitrarily different from the true counts), while all the labels in the test set were confirmed to be accurate. Unfortunately, the identities of the erroneous records in the training set are not available. Can this information about presence of 5% errors in the training set labels (without details about the specific identities of the erroneous rows) be used to improve the performance of the model in Part (g)? Note that we are interested in improving the $R^2$ performance of the model on the test set (not the training $R^2$ score). \n",
    "\n",
    "As a final task, we require you to come up with a strategy to fit a regression model, taking into account the errors in the training set labels. Explain the intuition behind your approach (we do not expect a detailed mathematical justification). Use your approach to fit a regression model on the training set, and compare its test $R^2$ with the model in Part (g).\n",
    "\n",
    "**Note:** Again, we do not require you to implement a new regression solver for handling erroneous labels. It is sufficient that you to come up with an approach that uses existing built-in functions. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
